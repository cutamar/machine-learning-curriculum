{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "## Matlab Resources\n",
    "### Onramp\n",
    "- Go to: https://matlabacademy.mathworks.com/ and click on the MATLAB Onramp button to start learning MATLAB\n",
    "\n",
    "### Tutorials\n",
    "#### Get Started with MATLAB and MATLAB Online\n",
    "- [What is MATLAB?](https://youtu.be/WYG2ZZjgp5M)\\*\n",
    "- [MATLAB Variables](https://youtu.be/0w9NKt6Fixk)\\*\n",
    "- [MATLAB as a Calculator](https://youtu.be/aRSkNpCSgWY)\\*\n",
    "- [MATLAB Functions](https://youtu.be/RJp46UVQBic)\\*\n",
    "- [Getting Started with MATLAB Online](https://youtu.be/XjzxCVWKz58)\n",
    "- [Managing Files in MATLAB Online](https://youtu.be/B3lWLIrYjC0)\n",
    "\n",
    "#### Vectors\n",
    "- [Creating Vectors](https://youtu.be/R5Mnkrk9Mos)\\*\n",
    "- [Creating Uniformly Spaced Vectors](https://youtu.be/_zqTOV5yl8Y)\\*\n",
    "- [Accessing Elements of a Vector Using Conditions](https://youtu.be/8D04GW_foQ0)\\*\n",
    "- [Calculations with Vectors](https://youtu.be/VQaZ0TvjF0c)\\*\n",
    "- [Vector Transpose](https://youtu.be/vgRLwjHBmsg)\n",
    "\n",
    "#### Visualization\n",
    "- [Line Plots](https://youtu.be/-hhJoveE4sY)\\*\n",
    "- [Annotating Graphs](https://youtu.be/JyovEGPSdoI)\\*\n",
    "- [Multiple Plots](https://youtu.be/fBx8EFuXFLM)\\*\n",
    "\n",
    "#### Matrices\n",
    "- [Creating Matrices](https://youtu.be/qdTdwTh6jMo)\\*\n",
    "- [Calculations with Matrices](https://youtu.be/mzzJ9gnMrYE)\\*\n",
    "- [Accessing Elements of a Matrix](https://youtu.be/uWPHxpTuZRA)\\*\n",
    "- [Matrix Creation Functions](https://youtu.be/VPcbpVd_mPA)\\*\n",
    "- [Combining Matrices](https://youtu.be/ejTr3ekTTyA)\n",
    "- [Determining Array Size and Length](https://youtu.be/IF9-ffmxuy8)\n",
    "- [Matrix Multiplication](https://youtu.be/4hsx3bdNjGk)\n",
    "- [Reshaping Arrays](https://youtu.be/UQpDIHlFo8A)\n",
    "- [Statistical Functions with Matrices](https://youtu.be/Y97W3_u7cM4)\n",
    "\n",
    "#### MATLAB Programming\n",
    "- [Logical Variables](https://youtu.be/bRMg4GsFDQ8)\\*\n",
    "- [If-Else Statements](https://youtu.be/JZSuU-Laigo)\\*\n",
    "- [Writing a FOR loop](https://youtu.be/lg65bzgvI5c)\\*\n",
    "- [Writing a WHILE Loop](https://youtu.be/PKH5lCMJXbk)\n",
    "- [Writing Functions](https://youtu.be/GrcNN04eqXU)\n",
    "- [Passing Functions as Inputs](https://youtu.be/aNCwR9dRjHs)\n",
    "\n",
    "#### Troubleshooting\n",
    "- [Using Online Documentation](https://youtu.be/54n5zJwR8aM)\\*\n",
    "- [Which File or Variable Am I Using?](https://youtu.be/Z09BvGeYNdE)\n",
    "- [Troubleshooting Code with the Debugger](https://youtu.be/DB4aJMnZtNQ)\n",
    "\n",
    "***Indicates content covered in Onramp**\n",
    "\n",
    "## Multivariate Linear Regression\n",
    "### Notation\n",
    "- $n$ = nubmer of features\n",
    "- $x^{(i)}$ = input (features) of $i^{th}$ training example\n",
    "- $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example\n",
    "\n",
    "### Hypothesis\n",
    "- For convenience of notation, we define $(x_0^{(i)}=1)$, so all $x_0$'s are equal to 1\n",
    "- $h_{\\theta}(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$\n",
    "- $x, \\theta \\in \\mathbb{R}^{n+1}$\n",
    "- This way it can be also written in vector form:\n",
    "    - $h_{\\theta}(x)=\\theta^T x=\\begin{bmatrix}\\theta_0 & \\theta_1 & \\cdots & \\theta_n\\end{bmatrix} \\cdot \\begin{bmatrix}x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n",
    "    \n",
    "### Cost Function\n",
    "$$J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "    \n",
    "### Gradient Descent $(n\\geq 1)$\n",
    "- Repeat the following for $j=0,\\dots, n$:\n",
    "$$\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "### Feature Scaling\n",
    "- Speeds gradient descent up, because $\\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven\n",
    "- To prevent this we modify the ranges to roughly the same\n",
    "    - $-1 \\leq x_{(i)} \\leq 1$ or\n",
    "    - $-0.5 \\leq x_{(i)} \\leq 0.5$\n",
    "- We use **feature scaling** (division part) and **mean normalization** (subtraction part):\n",
    "$$x_i := \\frac{x_i - \\mu_i}{s_i}$$\n",
    "- $\\mu_i$ is the **average** of all the values for feature (i)\n",
    "- $s_i$ is the **range** of values (max - min) or the **standard deviation**\n",
    "\n",
    "### Debugging Gradient Descent\n",
    "- Make a plot with *number of iterations* on the x-axis and plot the cost function $J(\\theta)$ over the number of iterations of gradient descent\n",
    "- If $J(\\theta)$ ever increases, then you probably need to decrease the learning rate $\\alpha$\n",
    "- **Automatic convergence tests** are also possible:\n",
    "    - You declare convergence if $J(\\theta)$ decreases by less than E in one iteration, where E is some small value such as $10^{-3}$\n",
    "    - However in practice it's difficult to choose E\n",
    "    \n",
    "### Features and Polynomial Regression\n",
    "- We can combine multiple features into one feature, e.g. width and height into one feature, area (= width x height)\n",
    "- Also our hypothesis function need not be linear, if that doesn't fit the data well\n",
    "- We could use a quadratic, cubic, square function etc.\n",
    "    - Square function example: $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 \\sqrt{x_1}$\n",
    "- Choosing features this way, don't forget that feature scaling becomes even more important\n",
    "\n",
    "### Normal Equation\n",
    "#### How it Works\n",
    "- Another way of minimizing $J$\n",
    "- Explicit, non-iterative, analytical way\n",
    "- We minimize $J$ by explicitly taking its derivatives with respect to the $\\theta_j$'s, and set them to zero\n",
    "$$\\theta = (X^T X)^{-1}X^Ty$$\n",
    "- Don't forget to add $x_0^{(i)}$ to the $X$ matrix (which equals 1)\n",
    "- There is **no need** to do feature scaling with the normal equation\n",
    "- Because it needs to calculate the inverse of $X^T X$, it's slow if $n$ is very large\n",
    "- As a broad rule, you should switch to gradient descend for $n \\geq 10000$\n",
    "- Normal equation has a runtime of $O(n^3)$, as compared to gradient descend, which has a runtime of $O(kn^2)$\n",
    "\n",
    "#### Noninvertibility\n",
    "- Using `pinv` in octave/matlab will give us a value of $\\theta$ even if $X^T X$ is not invertible (singular/degenerate)\n",
    "- If $X^T X$ is **noninvertible**, possible reasons are:\n",
    "    - Redundant features, where two features are very closely related (i.e. linearly dependent)\n",
    "    - Too many features (e.g. $m \\leq n$)\n",
    "        - In this case delete some features or\n",
    "        - Use **regularization**\n",
    "        \n",
    "## Octave/Matlab Commands\n",
    "### Basic Operations\n",
    "```\n",
    "% equal\n",
    "1 == 2\n",
    "\n",
    "% not equal\n",
    "1 ~= 2\n",
    "\n",
    "% and\n",
    "1 && 0\n",
    "\n",
    "% or\n",
    "1 || 0\n",
    "\n",
    "% xor\n",
    "xor(1,0)\n",
    "\n",
    "% change prompt\n",
    "PS1('>> ')\n",
    "\n",
    "% semicolon supresses output\n",
    "a = 3;\n",
    "\n",
    "% display (print)\n",
    "disp('Hello World')\n",
    "\n",
    "% string format\n",
    "sprintf('2 decimals: %0.2f', pi)\n",
    "\n",
    "% change how many digits should be shown\n",
    "format short\n",
    "format long\n",
    "\n",
    "% generate row vector start:step:end\n",
    "1:0.25:2\n",
    "\n",
    "% you can also leave out the step param i.e. start:end, this will by default increment by 1\n",
    "1:5\n",
    "\n",
    "% generate matrix consisting of ones (row count, column count)\n",
    "ones(2,3)\n",
    "\n",
    "% generate matrix consisting of zeros (row count, column count)\n",
    "zeros(1,3)\n",
    "\n",
    "% generate matrix consisting of random values between 0 and 1 (row count, column count)\n",
    "rand(1,3)\n",
    "\n",
    "% generate matrix consisting of normally distributed random values (row count, column count)\n",
    "randn(1,3)\n",
    "\n",
    "% plot histogram (data, optional: bin/bucket count) NOTE: in matlab histogram should be used instead of hist\n",
    "hist(randn(1, 10000))\n",
    "\n",
    "% generate identity matrix for the given dimension\n",
    "eye(6)\n",
    "\n",
    "% help for given function\n",
    "help eye\n",
    "```\n",
    "\n",
    "### Moving Data Around\n",
    "```\n",
    "% number of rows\n",
    "size(A, 1)\n",
    "\n",
    "% number of columns\n",
    "size(A, 2)\n",
    "\n",
    "% gives the size of the longest dimension, but usually only used on vectors\n",
    "length(A)\n",
    "\n",
    "% current working directory\n",
    "pwd\n",
    "\n",
    "% change directory\n",
    "cd\n",
    "\n",
    "% list files and folders\n",
    "ls\n",
    "\n",
    "% load data\n",
    "load featuresX.dat\n",
    "\n",
    "% or the same calling\n",
    "load('featuresX.dat')\n",
    "\n",
    "% shows variables in current scope\n",
    "who\n",
    "\n",
    "% or for the detailed view\n",
    "whos\n",
    "\n",
    "% remove variable from scope\n",
    "clear featuresX\n",
    "\n",
    "% get first 10 elements\n",
    "priceY(1:10)\n",
    "\n",
    "% saves variable v into file hello.mat\n",
    "save hello.mat v\n",
    "\n",
    "% clear all variables\n",
    "clear\n",
    "\n",
    "% saves in a human readable format (no metadata like variable name)\n",
    "save hello.txt v -ascii\n",
    "\n",
    "% fetch everything in the second row (\":\" means every element along that row/column)\n",
    "A(2,:)\n",
    "\n",
    "% fetch everything from first and third row\n",
    "A([1 3], :)\n",
    "\n",
    "% can also be used for assignments\n",
    "A(:,2) = [10; 11; 12]\n",
    "\n",
    "% append another column vector to right\n",
    "A = [A, [100; 101; 102]]\n",
    "\n",
    "% put all elements of A into a single vector\n",
    "A(:)\n",
    "\n",
    "% concat two matrices\n",
    "C = [A B]\n",
    "\n",
    "% or the same as\n",
    "C = [A, B]\n",
    "\n",
    "% or put it on the bottom\n",
    "C = [A; B]\n",
    "```\n",
    "\n",
    "### Computing on Data\n",
    "```\n",
    "% multiple A11 with B11, A12 with B12 etc. (element-wise)\n",
    "A .* B\n",
    "\n",
    "% element-wise squaring\n",
    "A .^ 2\n",
    "\n",
    "% element-wise inverse\n",
    "1 ./ A\n",
    "\n",
    "% element-wise log\n",
    "log(v)\n",
    "\n",
    "% element-wise exp\n",
    "exp(v)\n",
    "\n",
    "% element-wise abs\n",
    "abs(v)\n",
    "\n",
    "% same as -1*v\n",
    "-v\n",
    "\n",
    "% element-wise incremental by e.g. 1\n",
    "v + ones(length(v), 1)\n",
    "\n",
    "% or use this (+ and - are element-wise)\n",
    "v + 1\n",
    "\n",
    "% returns max value and index\n",
    "[val, ind] = max(a)\n",
    "\n",
    "% element-wise comparison\n",
    "a < 3\n",
    "\n",
    "% tells the indexes of the variables for which the condition is true\n",
    "find(a < 3)\n",
    "\n",
    "% generates a matrix of n x n dimension, where all rows, columns and diagonals sum up to the same value\n",
    "magic(3)\n",
    "\n",
    "% find used on matrices, returns rows and columns\n",
    "[r,c] = find(A >= 7)\n",
    "\n",
    "% adds up all elements\n",
    "sum(a)\n",
    "\n",
    "% product of all elements\n",
    "prod(a)\n",
    "\n",
    "% round down\n",
    "floor(a)\n",
    "\n",
    "% round up\n",
    "ceil(a)\n",
    "\n",
    "% element-wise max\n",
    "max(A, B)\n",
    "\n",
    "% column-wise max\n",
    "max(A,[],1)\n",
    "\n",
    "% or use\n",
    "max(A)\n",
    "\n",
    "% row-wise max\n",
    "max(A,[],2)\n",
    "\n",
    "% max element\n",
    "max(max(A))\n",
    "\n",
    "% or turn A into a vector\n",
    "max(A(:))\n",
    "\n",
    "% column-wise sum\n",
    "sum(A,1)\n",
    "\n",
    "% row-wise sum\n",
    "sum(A,2)\n",
    "\n",
    "% diagonal sum\n",
    "sum(sum(A .* eye(length(A))))\n",
    "\n",
    "% other diagonal sum\n",
    "sum(sum(A .* flipud(eye(length(A)))))\n",
    "```\n",
    "\n",
    "### Plotting Data\n",
    "```\n",
    "t=[0:0.01:0.98]\n",
    "\n",
    "% plot given x and y data\n",
    "plot(t, sin(t))\n",
    "\n",
    "% plots next figures on top of the open one (old one)\n",
    "hold on\n",
    "\n",
    "% sets x-axis label\n",
    "xlabel('time')\n",
    "\n",
    "% sets y-axis label\n",
    "ylabel('value')\n",
    "\n",
    "% show legend\n",
    "legend('sin', 'cos')\n",
    "\n",
    "% show title\n",
    "title('my plot')\n",
    "\n",
    "% saves open plot as png\n",
    "print -dpng 'myPlot.png'\n",
    "\n",
    "% close open plot\n",
    "close\n",
    "\n",
    "% multiple plots\n",
    "figure(1); plot(t, sin(t));\n",
    "figure(2); plot(t, cos(t));\n",
    "\n",
    "% divides plot into a 1x2 grid, access first element\n",
    "subplot(1,2,1)\n",
    "\n",
    "% set x-axis range to 0.5 -> 1 and y-axis range to -1 -> 1\n",
    "axis([0.5 1 -1 1])\n",
    "\n",
    "% clear plot\n",
    "clf\n",
    "\n",
    "% plot matrix\n",
    "imagesc(A)\n",
    "\n",
    "% show colorbar with values\n",
    "colorbar\n",
    "\n",
    "% change to gray colormap\n",
    "colormap gray\n",
    "\n",
    "% comma chaining of commands, useful e.g. if you want to change colormap etc. (output doesn't get surpressed like when using \";\")\n",
    "a=1, b=2, c=3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
